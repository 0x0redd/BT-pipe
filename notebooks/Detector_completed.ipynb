{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.5.1+cu121\n",
            "CUDA available: True\n",
            "CUDA version: 12.1\n",
            "GPU device: NVIDIA GeForce RTX 3060 Laptop GPU\n",
            "Number of GPUs: 1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(f'PyTorch version: {torch.__version__}')\n",
        "print(f'CUDA available: {torch.cuda.is_available()}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'CUDA version: {torch.version.cuda}')\n",
        "    print(f'GPU device: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'Number of GPUs: {torch.cuda.device_count()}')\n",
        "else:\n",
        "    print('CUDA not available - GPU support not working')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tumor Detector (Completed)\n",
        "\n",
        "Complete CNN/FasterRCNN/YOLOv8 training + validation notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment\n",
        "Install deps (PyTorch/torchvision, YAML, Pillow, optional ultralytics for YOLOv8).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# %pip install -q torch torchvision pyyaml pillow\n",
        "%pip install -q pyyaml pillow\n",
        "# Uncomment for YOLOv8\n",
        "%pip install -q ultralytics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Paths and config\n",
        "Set dataset locations and training hyperparameters. Adjust to your Task01-processed PNGs and JSONs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'experiment_name': 'detector_complete',\n",
              " 'task': 'detection',\n",
              " 'model': {'name': 'fasterrcnn_resnet50_fpn', 'pretrained': True},\n",
              " 'data': {'image_root': 'data\\\\processed',\n",
              "  'train_annotations': 'data\\\\annotations\\\\train.json',\n",
              "  'val_annotations': 'data\\\\annotations\\\\val.json',\n",
              "  'labels': ['No Tumor', 'Glioma', 'Meningioma', 'Pituitary']},\n",
              " 'train': {'epochs': 5,\n",
              "  'batch_size': 2,\n",
              "  'lr': 0.0002,\n",
              "  'weight_decay': 0.0005,\n",
              "  'num_workers': 2,\n",
              "  'grad_clip': 1.0,\n",
              "  'checkpoint_dir': 'runs/detector',\n",
              "  'save_every': 1}}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "import torch\n",
        "\n",
        "# Add project root so `models` imports work\n",
        "PROJECT_ROOT = Path.cwd().resolve()\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "# Update these to your data\n",
        "image_root = Path(\"data/processed\")  # folder with PNG/JPG slices\n",
        "train_annotations = Path(\"data/annotations/train.json\")\n",
        "val_annotations = Path(\"data/annotations/val.json\")\n",
        "\n",
        "default_labels = [\"No Tumor\", \"Glioma\", \"Meningioma\", \"Pituitary\"]\n",
        "\n",
        "config = {\n",
        "    \"experiment_name\": \"detector_complete\",\n",
        "    \"task\": \"detection\",  # 'classification' or 'detection'\n",
        "    \"model\": {\n",
        "        \"name\": \"fasterrcnn_resnet50_fpn\",  # or resnet18 for classification\n",
        "        \"pretrained\": True,\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"image_root\": str(image_root),\n",
        "        \"train_annotations\": str(train_annotations),\n",
        "        \"val_annotations\": str(val_annotations),\n",
        "        \"labels\": default_labels,\n",
        "    },\n",
        "    \"train\": {\n",
        "        \"epochs\": 5,\n",
        "        \"batch_size\": 2,\n",
        "        \"lr\": 2e-4,\n",
        "        \"weight_decay\": 5e-4,\n",
        "        \"num_workers\": 2,\n",
        "        \"grad_clip\": 1.0,\n",
        "        \"checkpoint_dir\": \"runs/detector\",\n",
        "        \"save_every\": 1,\n",
        "    },\n",
        "}\n",
        "\n",
        "# Save a copy of the config for reference\n",
        "Path(\"configs\").mkdir(parents=True, exist_ok=True)\n",
        "with open(\"configs/detector_completed.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
        "    yaml.safe_dump(config, f)\n",
        "\n",
        "config\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using project root for imports: C:\\Users\\FERRA\\OneDrive\\Documents\\github\\BT-pipe\n",
            "models exists: True\n",
            "Image root: C:\\Users\\FERRA\\OneDrive\\Documents\\github\\BT-pipe\\data\\processed\n",
            "Train annotations: C:\\Users\\FERRA\\OneDrive\\Documents\\github\\BT-pipe\\data\\annotations\\train.json\n",
            "Val annotations: C:\\Users\\FERRA\\OneDrive\\Documents\\github\\BT-pipe\\data\\annotations\\val.json\n"
          ]
        }
      ],
      "source": [
        "# Ensure repo root (with `models/`) is on sys.path\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Try current directory, then walk up to find 'models'\n",
        "root = Path.cwd().resolve()\n",
        "if not (root / \"models\").exists():\n",
        "    for parent in root.parents:\n",
        "        if (parent / \"models\").exists():\n",
        "            root = parent\n",
        "            break\n",
        "\n",
        "if str(root) not in sys.path:\n",
        "    sys.path.insert(0, str(root))\n",
        "\n",
        "# Override data paths to be absolute from project root\n",
        "image_root = root / \"data/processed\"\n",
        "train_annotations = root / \"data/annotations/train.json\"\n",
        "val_annotations = root / \"data/annotations/val.json\"\n",
        "config[\"data\"].update(\n",
        "    {\n",
        "        \"image_root\": str(image_root),\n",
        "        \"train_annotations\": str(train_annotations),\n",
        "        \"val_annotations\": str(val_annotations),\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Using project root for imports:\", root)\n",
        "print(\"models exists:\", (root / \"models\").exists())\n",
        "print(\"Image root:\", image_root)\n",
        "print(\"Train annotations:\", train_annotations)\n",
        "print(\"Val annotations:\", val_annotations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task01 converter (NIfTI â†’ PNG + JSON)\n",
        "Run once to generate `data/processed/*.png` and `data/annotations/train.json` / `val.json` from BRATS Task01 volumes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q nibabel scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found images: 484 labels: 484\n",
            "BRATS_001: 74 non-empty slices\n",
            "BRATS_002: 61 non-empty slices\n",
            "BRATS_003: 82 non-empty slices\n",
            "BRATS_004: 73 non-empty slices\n",
            "BRATS_005: 76 non-empty slices\n",
            "BRATS_006: 84 non-empty slices\n",
            "BRATS_007: 58 non-empty slices\n",
            "BRATS_008: 76 non-empty slices\n",
            "BRATS_009: 74 non-empty slices\n",
            "BRATS_010: 41 non-empty slices\n",
            "BRATS_011: 66 non-empty slices\n",
            "BRATS_012: 44 non-empty slices\n",
            "BRATS_013: 36 non-empty slices\n",
            "BRATS_014: 72 non-empty slices\n",
            "BRATS_015: 81 non-empty slices\n",
            "BRATS_016: 57 non-empty slices\n",
            "BRATS_017: 60 non-empty slices\n",
            "BRATS_018: 76 non-empty slices\n",
            "BRATS_019: 97 non-empty slices\n",
            "BRATS_020: 80 non-empty slices\n",
            "BRATS_021: 48 non-empty slices\n",
            "BRATS_022: 80 non-empty slices\n",
            "BRATS_023: 57 non-empty slices\n",
            "BRATS_024: 41 non-empty slices\n",
            "BRATS_025: 33 non-empty slices\n",
            "BRATS_026: 56 non-empty slices\n",
            "BRATS_027: 49 non-empty slices\n",
            "BRATS_028: 53 non-empty slices\n",
            "BRATS_029: 27 non-empty slices\n",
            "BRATS_030: 44 non-empty slices\n",
            "BRATS_031: 58 non-empty slices\n",
            "BRATS_032: 86 non-empty slices\n",
            "BRATS_033: 76 non-empty slices\n",
            "BRATS_034: 92 non-empty slices\n",
            "BRATS_035: 72 non-empty slices\n",
            "BRATS_036: 65 non-empty slices\n",
            "BRATS_037: 73 non-empty slices\n",
            "BRATS_038: 66 non-empty slices\n",
            "BRATS_039: 41 non-empty slices\n",
            "BRATS_040: 65 non-empty slices\n",
            "BRATS_041: 82 non-empty slices\n",
            "BRATS_042: 75 non-empty slices\n",
            "BRATS_043: 73 non-empty slices\n",
            "BRATS_044: 68 non-empty slices\n",
            "BRATS_045: 68 non-empty slices\n",
            "BRATS_046: 77 non-empty slices\n",
            "BRATS_047: 78 non-empty slices\n",
            "BRATS_048: 95 non-empty slices\n",
            "BRATS_049: 62 non-empty slices\n",
            "BRATS_050: 54 non-empty slices\n",
            "BRATS_051: 83 non-empty slices\n",
            "BRATS_052: 65 non-empty slices\n",
            "BRATS_053: 74 non-empty slices\n",
            "BRATS_054: 68 non-empty slices\n",
            "BRATS_055: 80 non-empty slices\n",
            "BRATS_056: 81 non-empty slices\n",
            "BRATS_057: 83 non-empty slices\n",
            "BRATS_058: 84 non-empty slices\n",
            "BRATS_059: 64 non-empty slices\n",
            "BRATS_060: 103 non-empty slices\n",
            "BRATS_061: 62 non-empty slices\n",
            "BRATS_062: 90 non-empty slices\n",
            "BRATS_063: 69 non-empty slices\n",
            "BRATS_064: 83 non-empty slices\n",
            "BRATS_065: 110 non-empty slices\n",
            "BRATS_066: 96 non-empty slices\n",
            "BRATS_067: 58 non-empty slices\n",
            "BRATS_068: 65 non-empty slices\n",
            "BRATS_069: 70 non-empty slices\n",
            "BRATS_070: 69 non-empty slices\n",
            "BRATS_071: 69 non-empty slices\n",
            "BRATS_072: 78 non-empty slices\n",
            "BRATS_073: 69 non-empty slices\n",
            "BRATS_074: 78 non-empty slices\n",
            "BRATS_075: 73 non-empty slices\n",
            "BRATS_076: 83 non-empty slices\n",
            "BRATS_077: 75 non-empty slices\n",
            "BRATS_078: 93 non-empty slices\n",
            "BRATS_079: 71 non-empty slices\n",
            "BRATS_080: 74 non-empty slices\n",
            "BRATS_081: 59 non-empty slices\n",
            "BRATS_082: 67 non-empty slices\n",
            "BRATS_083: 60 non-empty slices\n",
            "BRATS_084: 74 non-empty slices\n",
            "BRATS_085: 64 non-empty slices\n",
            "BRATS_086: 66 non-empty slices\n",
            "BRATS_087: 77 non-empty slices\n",
            "BRATS_088: 94 non-empty slices\n",
            "BRATS_089: 83 non-empty slices\n",
            "BRATS_090: 98 non-empty slices\n",
            "BRATS_091: 77 non-empty slices\n",
            "BRATS_092: 72 non-empty slices\n",
            "BRATS_093: 75 non-empty slices\n",
            "BRATS_094: 69 non-empty slices\n",
            "BRATS_095: 52 non-empty slices\n",
            "BRATS_096: 70 non-empty slices\n",
            "BRATS_097: 85 non-empty slices\n",
            "BRATS_098: 64 non-empty slices\n",
            "BRATS_099: 75 non-empty slices\n",
            "BRATS_100: 66 non-empty slices\n",
            "BRATS_101: 63 non-empty slices\n",
            "BRATS_102: 58 non-empty slices\n",
            "BRATS_103: 66 non-empty slices\n",
            "BRATS_104: 75 non-empty slices\n",
            "BRATS_105: 83 non-empty slices\n",
            "BRATS_106: 87 non-empty slices\n",
            "BRATS_107: 85 non-empty slices\n",
            "BRATS_108: 81 non-empty slices\n",
            "BRATS_109: 75 non-empty slices\n",
            "BRATS_110: 78 non-empty slices\n",
            "BRATS_111: 59 non-empty slices\n",
            "BRATS_112: 66 non-empty slices\n",
            "BRATS_113: 84 non-empty slices\n",
            "BRATS_114: 50 non-empty slices\n",
            "BRATS_115: 63 non-empty slices\n",
            "BRATS_116: 73 non-empty slices\n",
            "BRATS_117: 88 non-empty slices\n",
            "BRATS_118: 68 non-empty slices\n",
            "BRATS_119: 76 non-empty slices\n",
            "BRATS_120: 57 non-empty slices\n",
            "BRATS_121: 58 non-empty slices\n",
            "BRATS_122: 62 non-empty slices\n",
            "BRATS_123: 102 non-empty slices\n",
            "BRATS_124: 89 non-empty slices\n",
            "BRATS_125: 115 non-empty slices\n",
            "BRATS_126: 74 non-empty slices\n",
            "BRATS_127: 51 non-empty slices\n",
            "BRATS_128: 45 non-empty slices\n",
            "BRATS_129: 84 non-empty slices\n",
            "BRATS_130: 54 non-empty slices\n",
            "BRATS_131: 95 non-empty slices\n",
            "BRATS_132: 56 non-empty slices\n",
            "BRATS_133: 60 non-empty slices\n",
            "BRATS_134: 74 non-empty slices\n",
            "BRATS_135: 84 non-empty slices\n",
            "BRATS_136: 70 non-empty slices\n",
            "BRATS_137: 74 non-empty slices\n",
            "BRATS_138: 38 non-empty slices\n",
            "BRATS_139: 75 non-empty slices\n",
            "BRATS_140: 94 non-empty slices\n",
            "BRATS_141: 106 non-empty slices\n",
            "BRATS_142: 76 non-empty slices\n",
            "BRATS_143: 56 non-empty slices\n",
            "BRATS_144: 78 non-empty slices\n",
            "BRATS_145: 59 non-empty slices\n",
            "BRATS_146: 63 non-empty slices\n",
            "BRATS_147: 83 non-empty slices\n",
            "BRATS_148: 77 non-empty slices\n",
            "BRATS_149: 87 non-empty slices\n",
            "BRATS_150: 90 non-empty slices\n",
            "BRATS_151: 93 non-empty slices\n",
            "BRATS_152: 98 non-empty slices\n",
            "BRATS_153: 54 non-empty slices\n",
            "BRATS_154: 96 non-empty slices\n",
            "BRATS_155: 104 non-empty slices\n",
            "BRATS_156: 110 non-empty slices\n",
            "BRATS_157: 113 non-empty slices\n",
            "BRATS_158: 51 non-empty slices\n",
            "BRATS_159: 72 non-empty slices\n",
            "BRATS_160: 51 non-empty slices\n",
            "BRATS_161: 70 non-empty slices\n",
            "BRATS_162: 63 non-empty slices\n",
            "BRATS_163: 57 non-empty slices\n",
            "BRATS_164: 79 non-empty slices\n",
            "BRATS_165: 82 non-empty slices\n",
            "BRATS_166: 65 non-empty slices\n",
            "BRATS_167: 44 non-empty slices\n",
            "BRATS_168: 47 non-empty slices\n",
            "BRATS_169: 76 non-empty slices\n",
            "BRATS_170: 73 non-empty slices\n",
            "BRATS_171: 35 non-empty slices\n",
            "BRATS_172: 75 non-empty slices\n",
            "BRATS_173: 91 non-empty slices\n",
            "BRATS_174: 58 non-empty slices\n",
            "BRATS_175: 43 non-empty slices\n",
            "BRATS_176: 41 non-empty slices\n",
            "BRATS_177: 58 non-empty slices\n",
            "BRATS_178: 74 non-empty slices\n",
            "BRATS_179: 76 non-empty slices\n",
            "BRATS_180: 77 non-empty slices\n",
            "BRATS_181: 81 non-empty slices\n",
            "BRATS_182: 76 non-empty slices\n",
            "BRATS_183: 48 non-empty slices\n",
            "BRATS_184: 71 non-empty slices\n",
            "BRATS_185: 68 non-empty slices\n",
            "BRATS_186: 70 non-empty slices\n",
            "BRATS_187: 72 non-empty slices\n",
            "BRATS_188: 60 non-empty slices\n",
            "BRATS_189: 70 non-empty slices\n",
            "BRATS_190: 64 non-empty slices\n",
            "BRATS_191: 73 non-empty slices\n",
            "BRATS_192: 87 non-empty slices\n",
            "BRATS_193: 70 non-empty slices\n",
            "BRATS_194: 75 non-empty slices\n",
            "BRATS_195: 72 non-empty slices\n",
            "BRATS_196: 82 non-empty slices\n",
            "BRATS_197: 77 non-empty slices\n",
            "BRATS_198: 77 non-empty slices\n",
            "BRATS_199: 51 non-empty slices\n",
            "BRATS_200: 77 non-empty slices\n",
            "BRATS_201: 81 non-empty slices\n",
            "BRATS_202: 84 non-empty slices\n",
            "BRATS_203: 82 non-empty slices\n",
            "BRATS_204: 76 non-empty slices\n",
            "BRATS_205: 78 non-empty slices\n",
            "BRATS_206: 98 non-empty slices\n",
            "BRATS_207: 81 non-empty slices\n",
            "BRATS_208: 59 non-empty slices\n",
            "BRATS_209: 59 non-empty slices\n",
            "BRATS_210: 75 non-empty slices\n",
            "BRATS_211: 76 non-empty slices\n",
            "BRATS_212: 80 non-empty slices\n",
            "BRATS_213: 81 non-empty slices\n",
            "BRATS_214: 79 non-empty slices\n",
            "BRATS_215: 80 non-empty slices\n",
            "BRATS_216: 80 non-empty slices\n",
            "BRATS_217: 90 non-empty slices\n",
            "BRATS_218: 68 non-empty slices\n",
            "BRATS_219: 73 non-empty slices\n",
            "BRATS_220: 65 non-empty slices\n",
            "BRATS_221: 55 non-empty slices\n",
            "BRATS_222: 67 non-empty slices\n",
            "BRATS_223: 94 non-empty slices\n",
            "BRATS_224: 34 non-empty slices\n",
            "BRATS_225: 77 non-empty slices\n",
            "BRATS_226: 80 non-empty slices\n",
            "BRATS_227: 85 non-empty slices\n",
            "BRATS_228: 88 non-empty slices\n",
            "BRATS_229: 70 non-empty slices\n",
            "BRATS_230: 47 non-empty slices\n",
            "BRATS_231: 62 non-empty slices\n",
            "BRATS_232: 41 non-empty slices\n",
            "BRATS_233: 86 non-empty slices\n",
            "BRATS_234: 81 non-empty slices\n",
            "BRATS_235: 62 non-empty slices\n",
            "BRATS_236: 87 non-empty slices\n",
            "BRATS_237: 64 non-empty slices\n",
            "BRATS_238: 52 non-empty slices\n",
            "BRATS_239: 71 non-empty slices\n",
            "BRATS_240: 92 non-empty slices\n",
            "BRATS_241: 61 non-empty slices\n",
            "BRATS_242: 75 non-empty slices\n",
            "BRATS_243: 62 non-empty slices\n",
            "BRATS_244: 72 non-empty slices\n",
            "BRATS_245: 95 non-empty slices\n",
            "BRATS_246: 95 non-empty slices\n",
            "BRATS_247: 91 non-empty slices\n",
            "BRATS_248: 91 non-empty slices\n",
            "BRATS_249: 89 non-empty slices\n",
            "BRATS_250: 99 non-empty slices\n",
            "BRATS_251: 87 non-empty slices\n",
            "BRATS_252: 58 non-empty slices\n",
            "BRATS_253: 76 non-empty slices\n",
            "BRATS_254: 76 non-empty slices\n",
            "BRATS_255: 54 non-empty slices\n",
            "BRATS_256: 72 non-empty slices\n",
            "BRATS_257: 80 non-empty slices\n",
            "BRATS_258: 82 non-empty slices\n",
            "BRATS_259: 58 non-empty slices\n",
            "BRATS_260: 60 non-empty slices\n",
            "BRATS_261: 82 non-empty slices\n",
            "BRATS_262: 44 non-empty slices\n",
            "BRATS_263: 70 non-empty slices\n",
            "BRATS_264: 38 non-empty slices\n",
            "BRATS_265: 81 non-empty slices\n",
            "BRATS_266: 69 non-empty slices\n",
            "BRATS_267: 56 non-empty slices\n",
            "BRATS_268: 81 non-empty slices\n",
            "BRATS_269: 83 non-empty slices\n",
            "BRATS_270: 66 non-empty slices\n",
            "BRATS_271: 88 non-empty slices\n",
            "BRATS_272: 73 non-empty slices\n",
            "BRATS_273: 65 non-empty slices\n",
            "BRATS_274: 47 non-empty slices\n",
            "BRATS_275: 74 non-empty slices\n",
            "BRATS_276: 76 non-empty slices\n",
            "BRATS_277: 74 non-empty slices\n",
            "BRATS_278: 41 non-empty slices\n",
            "BRATS_279: 76 non-empty slices\n",
            "BRATS_280: 81 non-empty slices\n",
            "BRATS_281: 82 non-empty slices\n",
            "BRATS_282: 76 non-empty slices\n",
            "BRATS_283: 84 non-empty slices\n",
            "BRATS_284: 73 non-empty slices\n",
            "BRATS_285: 80 non-empty slices\n",
            "BRATS_286: 72 non-empty slices\n",
            "BRATS_287: 44 non-empty slices\n",
            "BRATS_288: 36 non-empty slices\n",
            "BRATS_289: 57 non-empty slices\n",
            "BRATS_290: 58 non-empty slices\n",
            "BRATS_291: 60 non-empty slices\n",
            "BRATS_292: 97 non-empty slices\n",
            "BRATS_293: 61 non-empty slices\n",
            "BRATS_294: 66 non-empty slices\n",
            "BRATS_295: 83 non-empty slices\n",
            "BRATS_296: 55 non-empty slices\n",
            "BRATS_297: 52 non-empty slices\n",
            "BRATS_298: 68 non-empty slices\n",
            "BRATS_299: 51 non-empty slices\n",
            "BRATS_300: 76 non-empty slices\n",
            "BRATS_301: 54 non-empty slices\n",
            "BRATS_302: 52 non-empty slices\n",
            "BRATS_303: 81 non-empty slices\n",
            "BRATS_304: 58 non-empty slices\n",
            "BRATS_305: 73 non-empty slices\n",
            "BRATS_306: 55 non-empty slices\n",
            "BRATS_307: 62 non-empty slices\n",
            "BRATS_308: 64 non-empty slices\n",
            "BRATS_309: 79 non-empty slices\n",
            "BRATS_310: 80 non-empty slices\n",
            "BRATS_311: 79 non-empty slices\n",
            "BRATS_312: 56 non-empty slices\n",
            "BRATS_313: 49 non-empty slices\n",
            "BRATS_314: 80 non-empty slices\n",
            "BRATS_315: 51 non-empty slices\n",
            "BRATS_316: 77 non-empty slices\n",
            "BRATS_317: 56 non-empty slices\n",
            "BRATS_318: 89 non-empty slices\n",
            "BRATS_319: 61 non-empty slices\n",
            "BRATS_320: 79 non-empty slices\n",
            "BRATS_321: 76 non-empty slices\n",
            "BRATS_322: 43 non-empty slices\n",
            "BRATS_323: 65 non-empty slices\n",
            "BRATS_324: 85 non-empty slices\n",
            "BRATS_325: 82 non-empty slices\n",
            "BRATS_326: 47 non-empty slices\n",
            "BRATS_327: 68 non-empty slices\n",
            "BRATS_328: 67 non-empty slices\n",
            "BRATS_329: 62 non-empty slices\n",
            "BRATS_330: 38 non-empty slices\n",
            "BRATS_331: 71 non-empty slices\n",
            "BRATS_332: 88 non-empty slices\n",
            "BRATS_333: 52 non-empty slices\n",
            "BRATS_334: 71 non-empty slices\n",
            "BRATS_335: 83 non-empty slices\n",
            "BRATS_336: 68 non-empty slices\n",
            "BRATS_337: 56 non-empty slices\n",
            "BRATS_338: 65 non-empty slices\n",
            "BRATS_339: 80 non-empty slices\n",
            "BRATS_340: 60 non-empty slices\n",
            "BRATS_341: 92 non-empty slices\n",
            "BRATS_342: 50 non-empty slices\n",
            "BRATS_343: 76 non-empty slices\n",
            "BRATS_344: 71 non-empty slices\n",
            "BRATS_345: 86 non-empty slices\n",
            "BRATS_346: 58 non-empty slices\n",
            "BRATS_347: 81 non-empty slices\n",
            "BRATS_348: 77 non-empty slices\n",
            "BRATS_349: 61 non-empty slices\n",
            "BRATS_350: 72 non-empty slices\n",
            "BRATS_351: 70 non-empty slices\n",
            "BRATS_352: 67 non-empty slices\n",
            "BRATS_353: 54 non-empty slices\n",
            "BRATS_354: 60 non-empty slices\n",
            "BRATS_355: 45 non-empty slices\n",
            "BRATS_356: 82 non-empty slices\n",
            "BRATS_357: 46 non-empty slices\n",
            "BRATS_358: 98 non-empty slices\n",
            "BRATS_359: 40 non-empty slices\n",
            "BRATS_360: 82 non-empty slices\n",
            "BRATS_361: 92 non-empty slices\n",
            "BRATS_362: 50 non-empty slices\n",
            "BRATS_363: 95 non-empty slices\n",
            "BRATS_364: 102 non-empty slices\n",
            "BRATS_365: 82 non-empty slices\n",
            "BRATS_366: 87 non-empty slices\n",
            "BRATS_367: 51 non-empty slices\n",
            "BRATS_368: 82 non-empty slices\n",
            "BRATS_369: 94 non-empty slices\n",
            "BRATS_370: 64 non-empty slices\n",
            "BRATS_371: 52 non-empty slices\n",
            "BRATS_372: 54 non-empty slices\n",
            "BRATS_373: 29 non-empty slices\n",
            "BRATS_374: 43 non-empty slices\n",
            "BRATS_375: 53 non-empty slices\n",
            "BRATS_376: 34 non-empty slices\n",
            "BRATS_377: 62 non-empty slices\n",
            "BRATS_378: 56 non-empty slices\n",
            "BRATS_379: 61 non-empty slices\n",
            "BRATS_380: 47 non-empty slices\n",
            "BRATS_381: 42 non-empty slices\n",
            "BRATS_382: 56 non-empty slices\n",
            "BRATS_383: 85 non-empty slices\n",
            "BRATS_384: 61 non-empty slices\n",
            "BRATS_385: 68 non-empty slices\n",
            "BRATS_386: 74 non-empty slices\n",
            "BRATS_387: 83 non-empty slices\n",
            "BRATS_388: 60 non-empty slices\n",
            "BRATS_389: 77 non-empty slices\n",
            "BRATS_390: 53 non-empty slices\n",
            "BRATS_391: 61 non-empty slices\n",
            "BRATS_392: 68 non-empty slices\n",
            "BRATS_393: 95 non-empty slices\n",
            "BRATS_394: 51 non-empty slices\n",
            "BRATS_395: 55 non-empty slices\n",
            "BRATS_396: 83 non-empty slices\n",
            "BRATS_397: 65 non-empty slices\n",
            "BRATS_398: 74 non-empty slices\n",
            "BRATS_399: 85 non-empty slices\n",
            "BRATS_400: 68 non-empty slices\n",
            "BRATS_401: 54 non-empty slices\n",
            "BRATS_402: 70 non-empty slices\n",
            "BRATS_403: 82 non-empty slices\n",
            "BRATS_404: 70 non-empty slices\n",
            "BRATS_405: 52 non-empty slices\n",
            "BRATS_406: 45 non-empty slices\n",
            "BRATS_407: 66 non-empty slices\n",
            "BRATS_408: 55 non-empty slices\n",
            "BRATS_409: 82 non-empty slices\n",
            "BRATS_410: 74 non-empty slices\n",
            "BRATS_411: 76 non-empty slices\n",
            "BRATS_412: 73 non-empty slices\n",
            "BRATS_413: 52 non-empty slices\n",
            "BRATS_414: 77 non-empty slices\n",
            "BRATS_415: 56 non-empty slices\n",
            "BRATS_416: 69 non-empty slices\n",
            "BRATS_417: 86 non-empty slices\n",
            "BRATS_418: 59 non-empty slices\n",
            "BRATS_419: 68 non-empty slices\n",
            "BRATS_420: 69 non-empty slices\n",
            "BRATS_421: 60 non-empty slices\n",
            "BRATS_422: 78 non-empty slices\n",
            "BRATS_423: 50 non-empty slices\n",
            "BRATS_424: 75 non-empty slices\n",
            "BRATS_425: 50 non-empty slices\n",
            "BRATS_426: 78 non-empty slices\n",
            "BRATS_427: 94 non-empty slices\n",
            "BRATS_428: 56 non-empty slices\n",
            "BRATS_429: 49 non-empty slices\n",
            "BRATS_430: 78 non-empty slices\n",
            "BRATS_431: 78 non-empty slices\n",
            "BRATS_432: 74 non-empty slices\n",
            "BRATS_433: 60 non-empty slices\n",
            "BRATS_434: 82 non-empty slices\n",
            "BRATS_435: 89 non-empty slices\n",
            "BRATS_436: 51 non-empty slices\n",
            "BRATS_437: 68 non-empty slices\n",
            "BRATS_438: 36 non-empty slices\n",
            "BRATS_439: 65 non-empty slices\n",
            "BRATS_440: 85 non-empty slices\n",
            "BRATS_441: 79 non-empty slices\n",
            "BRATS_442: 78 non-empty slices\n",
            "BRATS_443: 44 non-empty slices\n",
            "BRATS_444: 57 non-empty slices\n",
            "BRATS_445: 70 non-empty slices\n",
            "BRATS_446: 90 non-empty slices\n",
            "BRATS_447: 59 non-empty slices\n",
            "BRATS_448: 77 non-empty slices\n",
            "BRATS_449: 73 non-empty slices\n",
            "BRATS_450: 58 non-empty slices\n",
            "BRATS_451: 72 non-empty slices\n",
            "BRATS_452: 72 non-empty slices\n",
            "BRATS_453: 75 non-empty slices\n",
            "BRATS_454: 51 non-empty slices\n",
            "BRATS_455: 76 non-empty slices\n",
            "BRATS_456: 92 non-empty slices\n",
            "BRATS_457: 61 non-empty slices\n",
            "BRATS_458: 72 non-empty slices\n",
            "BRATS_459: 32 non-empty slices\n",
            "BRATS_460: 78 non-empty slices\n",
            "BRATS_461: 71 non-empty slices\n",
            "BRATS_462: 48 non-empty slices\n",
            "BRATS_463: 80 non-empty slices\n",
            "BRATS_464: 64 non-empty slices\n",
            "BRATS_465: 63 non-empty slices\n",
            "BRATS_466: 50 non-empty slices\n",
            "BRATS_467: 76 non-empty slices\n",
            "BRATS_468: 60 non-empty slices\n",
            "BRATS_469: 52 non-empty slices\n",
            "BRATS_470: 85 non-empty slices\n",
            "BRATS_471: 80 non-empty slices\n",
            "BRATS_472: 72 non-empty slices\n",
            "BRATS_473: 51 non-empty slices\n",
            "BRATS_474: 79 non-empty slices\n",
            "BRATS_475: 70 non-empty slices\n",
            "BRATS_476: 53 non-empty slices\n",
            "BRATS_477: 89 non-empty slices\n",
            "BRATS_478: 63 non-empty slices\n",
            "BRATS_479: 45 non-empty slices\n",
            "BRATS_480: 81 non-empty slices\n",
            "BRATS_481: 58 non-empty slices\n",
            "BRATS_482: 74 non-empty slices\n",
            "BRATS_483: 49 non-empty slices\n",
            "BRATS_484: 72 non-empty slices\n",
            "Total slices with tumors: 33438\n",
            "Wrote 26750 train and 6688 val samples\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Paths to your Task01 BRATS data (use project root)\n",
        "img_dir = root / \"Data/Task01_BrainTumour/imagesTr\"\n",
        "lbl_dir = root / \"Data/Task01_BrainTumour/labelsTr\"\n",
        "\n",
        "# Output locations for slices and annotations (absolute)\n",
        "out_images = root / \"data/processed\"\n",
        "out_ann = root / \"data/annotations\"\n",
        "out_images.mkdir(parents=True, exist_ok=True)\n",
        "out_ann.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Label list for config; using single-class \"Glioma\" here\n",
        "label_name = \"Glioma\"\n",
        "\n",
        "# Quick sanity checks - filter out macOS resource fork files\n",
        "img_files = [f for f in sorted(img_dir.glob(\"*.nii.gz\")) if not f.name.startswith(\"._\")]\n",
        "lbl_files = [f for f in sorted(lbl_dir.glob(\"*.nii.gz\")) if not f.name.startswith(\"._\")]\n",
        "# Also check for .nii files (without .gz)\n",
        "img_files_nii = [f for f in sorted(img_dir.glob(\"*.nii\")) if not f.name.startswith(\"._\") and not f.name.endswith(\".gz\")]\n",
        "lbl_files_nii = [f for f in sorted(lbl_dir.glob(\"*.nii\")) if not f.name.startswith(\"._\") and not f.name.endswith(\".gz\")]\n",
        "img_files.extend(img_files_nii)\n",
        "lbl_files.extend(lbl_files_nii)\n",
        "\n",
        "print(\"Found images:\", len(img_files), \"labels:\", len(lbl_files))\n",
        "if not img_files:\n",
        "    raise RuntimeError(\"No image volumes found at \" + str(img_dir))\n",
        "if not lbl_files:\n",
        "    raise RuntimeError(\"No label volumes found at \" + str(lbl_dir))\n",
        "\n",
        "def get_case_name(path: Path) -> str:\n",
        "    \"\"\"Extract case name from BRATS file, handling both .nii and .nii.gz\"\"\"\n",
        "    name = path.name\n",
        "    if name.endswith(\".nii.gz\"):\n",
        "        return name[:-7]  # Remove .nii.gz\n",
        "    elif name.endswith(\".nii\"):\n",
        "        return name[:-4]  # Remove .nii\n",
        "    return path.stem\n",
        "\n",
        "samples = []\n",
        "for img_path in img_files:\n",
        "    case = get_case_name(img_path)\n",
        "    # Try both .nii.gz and .nii extensions\n",
        "    lbl_path = lbl_dir / f\"{case}.nii.gz\"\n",
        "    if not lbl_path.exists():\n",
        "        lbl_path = lbl_dir / f\"{case}.nii\"\n",
        "    if not lbl_path.exists():\n",
        "        print(f\"Skipping {case}, missing label file (tried {case}.nii.gz and {case}.nii)\")\n",
        "        continue\n",
        "    img_vol = nib.load(img_path).get_fdata()\n",
        "    lbl_vol = nib.load(lbl_path).get_fdata()\n",
        "\n",
        "    # Use the first modality channel if 4D\n",
        "    if img_vol.ndim == 4:\n",
        "        img_vol = img_vol[..., 0]\n",
        "\n",
        "    # Count non-empty slices for this case\n",
        "    non_empty = int(np.sum(lbl_vol.max(axis=(0, 1)) > 0))\n",
        "    if non_empty == 0:\n",
        "        print(f\"All-empty label volume: {case}\")\n",
        "        continue\n",
        "    else:\n",
        "        print(f\"{case}: {non_empty} non-empty slices\")\n",
        "\n",
        "    for z in range(img_vol.shape[2]):\n",
        "        lbl_slice = lbl_vol[..., z]\n",
        "        if lbl_slice.max() < 1:  # skip empty slices\n",
        "            continue\n",
        "        ys, xs = np.where(lbl_slice > 0)\n",
        "        x1, x2 = xs.min(), xs.max()\n",
        "        y1, y2 = ys.min(), ys.max()\n",
        "\n",
        "        # Validate bounding box: ensure it has positive width and height\n",
        "        width = x2 - x1\n",
        "        height = y2 - y1\n",
        "        if width <= 0 or height <= 0:\n",
        "            continue  # Skip invalid bounding boxes\n",
        "\n",
        "        img_slice = img_vol[..., z]\n",
        "        # normalize to 0-255\n",
        "        arr = (img_slice - img_slice.min()) / (img_slice.max() - img_slice.min() + 1e-8)\n",
        "        arr = (arr * 255).clip(0, 255).astype(np.uint8)\n",
        "\n",
        "        png_name = f\"{case}_z{z:03d}.png\"\n",
        "        Image.fromarray(arr).save(out_images / png_name)\n",
        "\n",
        "        samples.append(\n",
        "            {\n",
        "                \"image\": png_name,\n",
        "                \"label\": label_name,\n",
        "                \"bbox\": [int(x1), int(y1), int(x2), int(y2)],\n",
        "            }\n",
        "        )\n",
        "\n",
        "print(\"Total slices with tumors:\", len(samples))\n",
        "\n",
        "if len(samples) == 0:\n",
        "    raise RuntimeError(\n",
        "        \"No labeled slices were found. Check that nibabel can read your label \"\n",
        "        \"volumes and that `labelsTr` contains non-empty masks.\"\n",
        "    )\n",
        "\n",
        "# Train/val split\n",
        "train, val = train_test_split(samples, test_size=0.2, random_state=42)\n",
        "with open(out_ann / \"train.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(train, f)\n",
        "with open(out_ann / \"val.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(val, f)\n",
        "\n",
        "print(\"Wrote\", len(train), \"train and\", len(val), \"val samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data loaders\n",
        "Uses the repo dataset utilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 484 image files and 484 label files\n",
            "Image dir: C:\\Users\\FERRA\\OneDrive\\Documents\\github\\BT-pipe\\Data\\Task01_BrainTumour\\imagesTr\n",
            "Label dir: C:\\Users\\FERRA\\OneDrive\\Documents\\github\\BT-pipe\\Data\\Task01_BrainTumour\\labelsTr\n",
            "First image file: BRATS_001.nii.gz\n",
            "First label file: BRATS_001.nii.gz\n",
            "Available label cases (first 10): ['BRATS_001', 'BRATS_002', 'BRATS_003', 'BRATS_004', 'BRATS_005', 'BRATS_006', 'BRATS_007', 'BRATS_008', 'BRATS_009', 'BRATS_010']\n",
            "\n",
            "Summary:\n",
            "  Matched image-label pairs: 484\n",
            "  Skipped (no label file): 0\n",
            "  Skipped (empty label volume): 0\n",
            "  Total slices with tumors: 33438\n",
            "Wrote 26750 train and 6688 val samples\n"
          ]
        }
      ],
      "source": [
        "import json, os\n",
        "from pathlib import Path\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use project root from cell 5 (ensure it's defined)\n",
        "try:\n",
        "    # root should be defined from cell 5\n",
        "    pass\n",
        "except NameError:\n",
        "    # Fallback if cell 5 wasn't run\n",
        "    root = Path.cwd().resolve()\n",
        "    if not (root / \"models\").exists():\n",
        "        for parent in root.parents:\n",
        "            if (parent / \"models\").exists():\n",
        "                root = parent\n",
        "                break\n",
        "\n",
        "img_dir = root / \"Data/Task01_BrainTumour/imagesTr\"\n",
        "lbl_dir = root / \"Data/Task01_BrainTumour/labelsTr\"\n",
        "out_images = root / \"data/processed\"\n",
        "out_ann = root / \"data/annotations\"\n",
        "out_images.mkdir(parents=True, exist_ok=True)\n",
        "out_ann.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Filter out macOS resource fork files and handle both .nii and .nii.gz\n",
        "img_files = [f for f in sorted(img_dir.glob(\"*.nii.gz\")) if not f.name.startswith(\"._\")]\n",
        "img_files_nii = [f for f in sorted(img_dir.glob(\"*.nii\")) if not f.name.startswith(\"._\") and not f.name.endswith(\".gz\")]\n",
        "img_files.extend(img_files_nii)\n",
        "\n",
        "# Get label files too\n",
        "lbl_files = [f for f in sorted(lbl_dir.glob(\"*.nii.gz\")) if not f.name.startswith(\"._\")]\n",
        "lbl_files_nii = [f for f in sorted(lbl_dir.glob(\"*.nii\")) if not f.name.startswith(\"._\") and not f.name.endswith(\".gz\")]\n",
        "lbl_files.extend(lbl_files_nii)\n",
        "\n",
        "print(f\"Found {len(img_files)} image files and {len(lbl_files)} label files\")\n",
        "print(f\"Image dir: {img_dir}\")\n",
        "print(f\"Label dir: {lbl_dir}\")\n",
        "if img_files:\n",
        "    print(f\"First image file: {img_files[0].name}\")\n",
        "if lbl_files:\n",
        "    print(f\"First label file: {lbl_files[0].name}\")\n",
        "\n",
        "def get_case_name(path: Path) -> str:\n",
        "    \"\"\"Extract case name from BRATS file, handling both .nii and .nii.gz\"\"\"\n",
        "    name = path.name\n",
        "    if name.endswith(\".nii.gz\"):\n",
        "        return name[:-7]  # Remove .nii.gz\n",
        "    elif name.endswith(\".nii\"):\n",
        "        return name[:-4]  # Remove .nii\n",
        "    return path.stem\n",
        "\n",
        "# Create a set of available label case names for faster lookup\n",
        "available_label_cases = {get_case_name(f) for f in lbl_files}\n",
        "print(f\"Available label cases (first 10): {sorted(list(available_label_cases))[:10]}\")\n",
        "\n",
        "samples = []\n",
        "matched = 0\n",
        "skipped_no_label = 0\n",
        "skipped_empty = 0\n",
        "\n",
        "for img_path in img_files:\n",
        "    case = get_case_name(img_path)\n",
        "    # Try both .nii.gz and .nii extensions\n",
        "    lbl_path = lbl_dir / f\"{case}.nii.gz\"\n",
        "    if not lbl_path.exists():\n",
        "        lbl_path = lbl_dir / f\"{case}.nii\"\n",
        "    if not lbl_path.exists():\n",
        "        skipped_no_label += 1\n",
        "        if skipped_no_label <= 5:  # Show first 5 mismatches\n",
        "            print(f\"  No label found for {case} (tried {case}.nii.gz and {case}.nii)\")\n",
        "        continue\n",
        "    matched += 1\n",
        "    img_vol = nib.load(img_path).get_fdata()\n",
        "    lbl_vol = nib.load(lbl_path).get_fdata()\n",
        "    # use one modality (e.g., FLAIR channel 0); adjust if needed\n",
        "    if img_vol.ndim == 4:\n",
        "        img_vol = img_vol[..., 0]\n",
        "\n",
        "    # Check if this volume has any non-empty slices\n",
        "    has_tumors = lbl_vol.max() > 0\n",
        "    if not has_tumors:\n",
        "        skipped_empty += 1\n",
        "        continue\n",
        "\n",
        "    for z in range(img_vol.shape[2]):\n",
        "        img_slice = img_vol[..., z]\n",
        "        lbl_slice = lbl_vol[..., z]\n",
        "        if lbl_slice.max() < 1:  # skip empty\n",
        "            continue\n",
        "        ys, xs = np.where(lbl_slice > 0)\n",
        "        x1, x2 = xs.min(), xs.max()\n",
        "        y1, y2 = ys.min(), ys.max()\n",
        "        \n",
        "        # Validate bounding box: ensure it has positive width and height\n",
        "        width = x2 - x1\n",
        "        height = y2 - y1\n",
        "        if width <= 0 or height <= 0:\n",
        "            continue  # Skip invalid bounding boxes\n",
        "        \n",
        "        # normalize to 0-255\n",
        "        arr = img_slice\n",
        "        arr = (arr - arr.min()) / (arr.max() - arr.min() + 1e-8)\n",
        "        arr = (arr * 255).clip(0, 255).astype(np.uint8)\n",
        "        png_name = f\"{case}_z{z:03d}.png\"\n",
        "        Image.fromarray(arr).save(out_images / png_name)\n",
        "        samples.append({\n",
        "            \"image\": png_name,\n",
        "            \"label\": \"Glioma\",           # set your class name\n",
        "            \"bbox\": [int(x1), int(y1), int(x2), int(y2)],\n",
        "        })\n",
        "\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"  Matched image-label pairs: {matched}\")\n",
        "print(f\"  Skipped (no label file): {skipped_no_label}\")\n",
        "print(f\"  Skipped (empty label volume): {skipped_empty}\")\n",
        "print(f\"  Total slices with tumors: {len(samples)}\")\n",
        "\n",
        "if len(samples) == 0:\n",
        "    print(f\"\\nDebugging info:\")\n",
        "    print(f\"  Image files found: {len(img_files)}\")\n",
        "    print(f\"  Label files found: {len(lbl_files)}\")\n",
        "    if img_files and lbl_files:\n",
        "        img_case = get_case_name(img_files[0])\n",
        "        lbl_case = get_case_name(lbl_files[0])\n",
        "        print(f\"  First image case: '{img_case}'\")\n",
        "        print(f\"  First label case: '{lbl_case}'\")\n",
        "        print(f\"  Do they match? {img_case == lbl_case}\")\n",
        "    raise RuntimeError(\n",
        "        \"No labeled slices were found. Check that nibabel can read your label \"\n",
        "        \"volumes and that `labelsTr` contains non-empty masks.\"\n",
        "    )\n",
        "\n",
        "# split train/val\n",
        "train, val = train_test_split(samples, test_size=0.2, random_state=42)\n",
        "json.dump(train, open(out_ann / \"train.json\", \"w\"))\n",
        "json.dump(val, open(out_ann / \"val.json\", \"w\"))\n",
        "print(\"Wrote\", len(train), \"train and\", len(val), \"val samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 26750 | Val samples: 6688\n",
            "Sample[0] keys: dict_keys(['image', 'label', 'bbox'])\n"
          ]
        }
      ],
      "source": [
        "from models.detector import TumorDataset, detection_collate_fn, build_model\n",
        "from models.detector.dataset import _build_transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Resolve paths and verify that annotation files exist\n",
        "from pathlib import Path\n",
        "\n",
        "def _resolve_with_fallback(p: Path) -> Path:\n",
        "    \"\"\"Resolve a path; if missing, search parents for a matching relative path.\"\"\"\n",
        "    p = p.expanduser()\n",
        "    if p.is_absolute():\n",
        "        if p.exists():\n",
        "            return p\n",
        "        # If absolute but under notebooks/, try parent project\n",
        "        candidates = [p, Path.cwd().resolve().parent / p.relative_to(p.anchor)] if \"notebooks\" in str(p) else [p]\n",
        "    else:\n",
        "        candidates = [Path.cwd() / p]\n",
        "    # Walk up parents to find matching relative location\n",
        "    rel = p.name if p.is_absolute() else p\n",
        "    for base in [Path.cwd()] + list(Path.cwd().parents):\n",
        "        candidate = base / rel\n",
        "        candidates.append(candidate)\n",
        "    for c in candidates:\n",
        "        try:\n",
        "            if c.exists():\n",
        "                return c.resolve()\n",
        "        except OSError:\n",
        "            continue\n",
        "    return p.resolve()\n",
        "\n",
        "train_ann = _resolve_with_fallback(Path(config[\"data\"][\"train_annotations\"]))\n",
        "val_ann = _resolve_with_fallback(Path(config[\"data\"][\"val_annotations\"]))\n",
        "img_root = _resolve_with_fallback(Path(config[\"data\"][\"image_root\"]))\n",
        "config[\"data\"].update(\n",
        "    {\n",
        "        \"train_annotations\": str(train_ann),\n",
        "        \"val_annotations\": str(val_ann),\n",
        "        \"image_root\": str(img_root),\n",
        "    }\n",
        ")\n",
        "\n",
        "for p, label in [(train_ann, \"train\"), (val_ann, \"val\")]:\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"Missing {label} annotations at {p}. Run the converter cell (Task01 -> PNG/JSON) to generate them.\"\n",
        "        )\n",
        "if not img_root.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Image root not found at {img_root}. Ensure the converter wrote PNGs there.\"\n",
        "    )\n",
        "\n",
        "# Build datasets\n",
        "train_ds = TumorDataset(\n",
        "    annotation_path=str(train_ann),\n",
        "    image_root=str(img_root),\n",
        "    labels=config[\"data\"][\"labels\"],\n",
        "    task=config[\"task\"],\n",
        "    transforms=_build_transforms(config[\"task\"], is_train=True),\n",
        ")\n",
        "val_ds = TumorDataset(\n",
        "    annotation_path=str(val_ann),\n",
        "    image_root=str(img_root),\n",
        "    labels=config[\"data\"][\"labels\"],\n",
        "    task=config[\"task\"],\n",
        "    transforms=_build_transforms(config[\"task\"], is_train=False),\n",
        ")\n",
        "\n",
        "collate_fn = detection_collate_fn if config[\"task\"] == \"detection\" else None\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=config[\"train\"][\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    num_workers=config[\"train\"][\"num_workers\"],\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=config[\"train\"][\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    num_workers=config[\"train\"][\"num_workers\"],\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "print(f\"Train samples: {len(train_ds)} | Val samples: {len(val_ds)}\")\n",
        "print(\"Sample[0] keys:\", train_ds.samples[0].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Build model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: fasterrcnn_resnet50_fpn | Task: detection | Device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = len(config[\"data\"][\"labels\"])\n",
        "model = build_model(\n",
        "    task=config[\"task\"],\n",
        "    num_classes=num_classes,\n",
        "    model_name=config[\"model\"][\"name\"],\n",
        "    pretrained=config[\"model\"][\"pretrained\"],\n",
        ").to(device)\n",
        "print(f\"Model: {config['model']['name']} | Task: {config['task']} | Device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training + validation\n",
        "Lightweight loop for both classification (acc/F1) and detection (loss proxy).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 260\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Uncomment to run training once config and data are set\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[20], line 191\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[1;34m(model, train_loader, val_loader, config, device)\u001b[0m\n\u001b[0;32m    188\u001b[0m epoch_loss_components \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m    189\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 191\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    192\u001b[0m     images \u001b[38;5;241m=\u001b[39m [img\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m    193\u001b[0m     targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n",
            "File \u001b[1;32mc:\\Users\\FERRA\\.conda\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\FERRA\\.conda\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\FERRA\\.conda\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1138\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1131\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1138\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
            "File \u001b[1;32mc:\\Users\\FERRA\\.conda\\envs\\dl\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\FERRA\\.conda\\envs\\dl\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\FERRA\\.conda\\envs\\dl\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\FERRA\\.conda\\envs\\dl\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\FERRA\\.conda\\envs\\dl\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "# Set up matplotlib for inline plotting\n",
        "%matplotlib inline\n",
        "# Try different style options for compatibility\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "except OSError:\n",
        "    try:\n",
        "        plt.style.use('seaborn-darkgrid')\n",
        "    except OSError:\n",
        "        plt.style.use('ggplot')\n",
        "\n",
        "def plot_training_curves(history, save_path=None):\n",
        "    \"\"\"Plot training curves for loss and accuracy/metrics\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Training Progress', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    \n",
        "    # Plot 1: Loss curves\n",
        "    ax1 = axes[0, 0]\n",
        "    ax1.plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
        "    if 'val_loss' in history:\n",
        "        ax1.plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "    ax1.set_xlabel('Epoch', fontsize=12)\n",
        "    ax1.set_ylabel('Loss', fontsize=12)\n",
        "    ax1.set_title('Loss Evolution', fontsize=14, fontweight='bold')\n",
        "    ax1.legend(fontsize=11)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Accuracy curves (for classification)\n",
        "    ax2 = axes[0, 1]\n",
        "    if 'train_acc' in history:\n",
        "        ax2.plot(epochs, history['train_acc'], 'b-', label='Train Accuracy', linewidth=2)\n",
        "        if 'val_acc' in history:\n",
        "            ax2.plot(epochs, history['val_acc'], 'r-', label='Val Accuracy', linewidth=2)\n",
        "        ax2.set_xlabel('Epoch', fontsize=12)\n",
        "        ax2.set_ylabel('Accuracy', fontsize=12)\n",
        "        ax2.set_title('Accuracy Evolution', fontsize=14, fontweight='bold')\n",
        "        ax2.legend(fontsize=11)\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "    else:\n",
        "        ax2.text(0.5, 0.5, 'Accuracy metrics\\nnot available\\nfor detection task', \n",
        "                ha='center', va='center', fontsize=12, transform=ax2.transAxes)\n",
        "        ax2.set_title('Accuracy Evolution', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    # Plot 3: Detection loss components (for detection task)\n",
        "    ax3 = axes[1, 0]\n",
        "    if 'loss_components' in history and history['loss_components']:\n",
        "        loss_comp = history['loss_components']\n",
        "        for comp_name, values in loss_comp.items():\n",
        "            if values:  # Check if list is not empty\n",
        "                ax3.plot(epochs, values, label=comp_name.replace('loss_', '').title(), linewidth=2)\n",
        "        ax3.set_xlabel('Epoch', fontsize=12)\n",
        "        ax3.set_ylabel('Loss', fontsize=12)\n",
        "        ax3.set_title('Detection Loss Components', fontsize=14, fontweight='bold')\n",
        "        ax3.legend(fontsize=10)\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "    else:\n",
        "        ax3.text(0.5, 0.5, 'Loss components\\nnot tracked', \n",
        "                ha='center', va='center', fontsize=12, transform=ax3.transAxes)\n",
        "        ax3.set_title('Loss Components', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    # Plot 4: Learning rate (if tracked)\n",
        "    ax4 = axes[1, 1]\n",
        "    if 'learning_rate' in history and history['learning_rate']:\n",
        "        ax4.plot(epochs, history['learning_rate'], 'g-', linewidth=2)\n",
        "        ax4.set_xlabel('Epoch', fontsize=12)\n",
        "        ax4.set_ylabel('Learning Rate', fontsize=12)\n",
        "        ax4.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
        "        ax4.set_yscale('log')\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "    else:\n",
        "        ax4.text(0.5, 0.5, 'Learning rate\\nnot tracked', \n",
        "                ha='center', va='center', fontsize=12, transform=ax4.transAxes)\n",
        "        ax4.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"Saved training curves to {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, config, device):\n",
        "    task = config['task']\n",
        "    epochs = config['train']['epochs']\n",
        "    lr = config['train']['lr']\n",
        "    weight_decay = config['train']['weight_decay']\n",
        "    grad_clip = config['train'].get('grad_clip', None)\n",
        "    \n",
        "    # Initialize history tracking\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_acc': [],\n",
        "        'loss_components': defaultdict(list),\n",
        "        'learning_rate': []\n",
        "    }\n",
        "    \n",
        "    # Create output directory for plots\n",
        "    output_dir = Path(config['train'].get('checkpoint_dir', 'runs/detector'))\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if task == 'classification':\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        \n",
        "        for epoch in range(1, epochs + 1):\n",
        "            model.train()\n",
        "            total_loss = correct = total = 0\n",
        "            for images, labels in train_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                if grad_clip:\n",
        "                    clip_grad_norm_(model.parameters(), grad_clip)\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item() * images.size(0)\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "            train_loss = total_loss / total\n",
        "            train_acc = correct / total\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['train_acc'].append(train_acc)\n",
        "            history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "            # validation\n",
        "            model.eval()\n",
        "            v_loss = v_correct = v_total = 0\n",
        "            y_true, y_pred = [], []\n",
        "            with torch.no_grad():\n",
        "                for images, labels in val_loader:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    v_loss += loss.item() * images.size(0)\n",
        "                    preds = outputs.argmax(dim=1)\n",
        "                    v_correct += (preds == labels).sum().item()\n",
        "                    v_total += labels.size(0)\n",
        "                    y_true.extend(labels.cpu().tolist())\n",
        "                    y_pred.extend(preds.cpu().tolist())\n",
        "            val_loss = v_loss / v_total\n",
        "            val_acc = v_correct / v_total\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['val_acc'].append(val_acc)\n",
        "            \n",
        "            print(f\"Epoch {epoch}/{epochs}: train_loss={train_loss:.4f} train_acc={train_acc:.4f} val_loss={val_loss:.4f} val_acc={val_acc:.4f}\")\n",
        "            \n",
        "            # Update plots every epoch\n",
        "            if epoch % 1 == 0:  # Update every epoch\n",
        "                plot_training_curves(history, save_path=output_dir / f'training_curves_epoch_{epoch}.png')\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"Final Classification Report:\")\n",
        "        print(\"=\"*60)\n",
        "        print(classification_report(y_true, y_pred, target_names=config['data']['labels']))\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(confusion_matrix(y_true, y_pred))\n",
        "        \n",
        "        # Final plot\n",
        "        plot_training_curves(history, save_path=output_dir / 'training_curves_final.png')\n",
        "\n",
        "    else:  # detection\n",
        "        params = [p for p in model.parameters() if p.requires_grad]\n",
        "        optimizer = optim.AdamW(params, lr=lr, weight_decay=weight_decay)\n",
        "        \n",
        "        for epoch in range(1, epochs + 1):\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            epoch_loss_components = defaultdict(float)\n",
        "            num_batches = 0\n",
        "            \n",
        "            for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "                images = [img.to(device) for img in images]\n",
        "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "                optimizer.zero_grad()\n",
        "                loss_dict = model(images, targets)\n",
        "                loss = sum(loss for loss in loss_dict.values())\n",
        "                loss.backward()\n",
        "                if grad_clip:\n",
        "                    clip_grad_norm_(model.parameters(), grad_clip)\n",
        "                optimizer.step()\n",
        "                \n",
        "                running_loss += loss.item()\n",
        "                for key, value in loss_dict.items():\n",
        "                    epoch_loss_components[key] += value.item()\n",
        "                num_batches += 1\n",
        "                \n",
        "                # Print progress every 100 batches\n",
        "                if (batch_idx + 1) % 100 == 0:\n",
        "                    print(f\"  Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "            avg_train_loss = running_loss / len(train_loader)\n",
        "            history['train_loss'].append(avg_train_loss)\n",
        "            \n",
        "            # Store loss components\n",
        "            for key, value in epoch_loss_components.items():\n",
        "                history['loss_components'][key].append(value / num_batches)\n",
        "            \n",
        "            history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "            # validation\n",
        "            val_loss = 0.0\n",
        "            val_loss_components = defaultdict(float)\n",
        "            val_batches = 0\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for images, targets in val_loader:\n",
        "                    images = [img.to(device) for img in images]\n",
        "                    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "                    loss_dict = model(images, targets)\n",
        "                    loss = sum(loss for loss in loss_dict.values())\n",
        "                    val_loss += loss.item()\n",
        "                    for key, value in loss_dict.items():\n",
        "                        val_loss_components[key] += value.item()\n",
        "                    val_batches += 1\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            history['val_loss'].append(avg_val_loss)\n",
        "            \n",
        "            # Print detailed loss breakdown\n",
        "            loss_str = \", \".join([f\"{k}={v/val_batches:.4f}\" for k, v in val_loss_components.items()])\n",
        "            print(f\"Epoch {epoch}/{epochs}: train_loss={avg_train_loss:.4f}, val_loss={avg_val_loss:.4f}\")\n",
        "            print(f\"  Val loss components: {loss_str}\")\n",
        "            \n",
        "            # Update plots every epoch\n",
        "            if epoch % 1 == 0:  # Update every epoch\n",
        "                plot_training_curves(history, save_path=output_dir / f'training_curves_epoch_{epoch}.png')\n",
        "        \n",
        "        # Final plot\n",
        "        plot_training_curves(history, save_path=output_dir / 'training_curves_final.png')\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"Training completed!\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Final train loss: {history['train_loss'][-1]:.4f}\")\n",
        "        print(f\"Final val loss: {history['val_loss'][-1]:.4f}\")\n",
        "    \n",
        "    return history\n",
        "\n",
        "# Uncomment to run training once config and data are set\n",
        "history = train_and_validate(model, train_loader, val_loader, config, device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.1. Visualization Helpers\n",
        "\n",
        "Helper functions to visualize training progress and model predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_detections(model, dataset, config, device, num_samples=6, threshold=0.5):\n",
        "    \"\"\"Visualize model predictions on validation samples\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Get random samples from dataset\n",
        "    indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle('Sample Detection Predictions', fontsize=16, fontweight='bold')\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, ax in zip(indices, axes):\n",
        "            image, target = dataset[idx]\n",
        "            \n",
        "            # Get prediction\n",
        "            img_tensor = image.unsqueeze(0).to(device)\n",
        "            outputs = model([img_tensor])[0]\n",
        "            \n",
        "            # Convert image tensor to numpy for display\n",
        "            img_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "            img_np = np.clip(img_np, 0, 1)\n",
        "            \n",
        "            ax.imshow(img_np, cmap='gray' if img_np.shape[2] == 1 else None)\n",
        "            ax.set_title(f'Sample {idx}', fontsize=12, fontweight='bold')\n",
        "            ax.axis('off')\n",
        "            \n",
        "            # Draw ground truth boxes\n",
        "            gt_boxes = target['boxes'].cpu().numpy()\n",
        "            for box in gt_boxes:\n",
        "                x1, y1, x2, y2 = box\n",
        "                rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, \n",
        "                                    fill=False, color='green', linewidth=2, label='Ground Truth')\n",
        "                ax.add_patch(rect)\n",
        "            \n",
        "            # Draw predicted boxes\n",
        "            pred_boxes = outputs['boxes'].cpu().numpy()\n",
        "            pred_scores = outputs['scores'].cpu().numpy()\n",
        "            pred_labels = outputs['labels'].cpu().numpy()\n",
        "            \n",
        "            for box, score, label in zip(pred_boxes, pred_scores, pred_labels):\n",
        "                if score >= threshold:\n",
        "                    x1, y1, x2, y2 = box\n",
        "                    label_name = config['data']['labels'][label]\n",
        "                    rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, \n",
        "                                        fill=False, color='red', linewidth=2, linestyle='--')\n",
        "                    ax.add_patch(rect)\n",
        "                    ax.text(x1, y1-5, f'{label_name}: {score:.2f}', \n",
        "                           color='red', fontsize=10, fontweight='bold',\n",
        "                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
        "            \n",
        "            # Add legend only to first subplot\n",
        "            if idx == indices[0]:\n",
        "                from matplotlib.patches import Patch\n",
        "                legend_elements = [\n",
        "                    Patch(facecolor='none', edgecolor='green', linewidth=2, label='Ground Truth'),\n",
        "                    Patch(facecolor='none', edgecolor='red', linewidth=2, linestyle='--', label='Prediction')\n",
        "                ]\n",
        "                ax.legend(handles=legend_elements, loc='upper right', fontsize=9)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    model.train()  # Set back to training mode\n",
        "\n",
        "\n",
        "def plot_confusion_matrix_heatmap(y_true, y_pred, labels, save_path=None):\n",
        "    \"\"\"Plot confusion matrix as a heatmap\"\"\"\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    \n",
        "    try:\n",
        "        import seaborn as sns\n",
        "        use_seaborn = True\n",
        "    except ImportError:\n",
        "        use_seaborn = False\n",
        "    \n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    if use_seaborn:\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                    xticklabels=labels, yticklabels=labels,\n",
        "                    cbar_kws={'label': 'Count'})\n",
        "    else:\n",
        "        # Fallback to matplotlib if seaborn not available\n",
        "        plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
        "        plt.colorbar(label='Count')\n",
        "        tick_marks = np.arange(len(labels))\n",
        "        plt.xticks(tick_marks, labels, rotation=45)\n",
        "        plt.yticks(tick_marks, labels)\n",
        "        for i in range(len(labels)):\n",
        "            for j in range(len(labels)):\n",
        "                plt.text(j, i, str(cm[i, j]), ha='center', va='center', fontweight='bold')\n",
        "    \n",
        "    plt.title('Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"Saved confusion matrix to {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "print(\"Visualization helpers loaded!\")\n",
        "print(\"\\nUsage examples:\")\n",
        "print(\"  # Visualize detection predictions:\")\n",
        "print(\"  visualize_detections(model, val_ds, config, device, num_samples=6)\")\n",
        "print(\"  # Plot confusion matrix (for classification):\")\n",
        "print(\"  plot_confusion_matrix_heatmap(y_true, y_pred, config['data']['labels'])\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def save_checkpoint(model, path=\"runs/detector/completed.pt\"):\n",
        "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Saved {path}\")\n",
        "\n",
        "save_checkpoint(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. YOLOv8 (optional detection)\n",
        "Requires images/labels in YOLO format and `ultralytics` installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote configs/yolo_task01.yaml\n"
          ]
        }
      ],
      "source": [
        "# Build YOLO data yaml (adjust paths/names)\n",
        "import yaml\n",
        "\n",
        "yolo_data = {\n",
        "    \"path\": \"data/yolo\",      # root containing images/ and labels/\n",
        "    \"train\": \"images/train\",\n",
        "    \"val\": \"images/val\",\n",
        "    \"names\": {0: \"Tumor\"},     # change if multiple classes\n",
        "}\n",
        "\n",
        "with open(\"configs/yolo_task01.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
        "    yaml.safe_dump(yolo_data, f)\n",
        "print(\"Wrote configs/yolo_task01.yaml\")\n",
        "\n",
        "# Example training (uncomment when data ready)\n",
        "from ultralytics import YOLO\n",
        "yolo_model = YOLO(\"yolov8n.pt\")\n",
        "yolo_model.train(data=\"configs/yolo_task01.yaml\", epochs=20, imgsz=640, batch=8, project=\"runs/yolo\", name=\"task01\")\n",
        "yolo_model.val(data=\"configs/yolo_task01.yaml\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Quick inference helper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "\n",
        "def infer_image(model, image_path, config, device):\n",
        "    model.eval()\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    if config['task'] == 'classification':\n",
        "        tfm = T.Compose([\n",
        "            T.Resize((224, 224)),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "        tensor = tfm(img).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(tensor)\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "            conf, pred = probs.max(dim=1)\n",
        "        return {\"label\": config['data']['labels'][pred.item()], \"confidence\": conf.item()}\n",
        "    else:\n",
        "        tfm = T.Compose([T.ToTensor()])\n",
        "        tensor = tfm(img).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model([tensor])[0]\n",
        "        if len(outputs['boxes']) == 0:\n",
        "            return {\"detections\": []}\n",
        "        detections = []\n",
        "        for b, s, l in zip(outputs['boxes'].cpu(), outputs['scores'].cpu(), outputs['labels'].cpu()):\n",
        "            detections.append({\n",
        "                \"bbox\": [round(x.item(), 2) for x in b],\n",
        "                \"score\": round(float(s), 4),\n",
        "                \"label\": config['data']['labels'][l],\n",
        "            })\n",
        "        return {\"detections\": detections}\n",
        "\n",
        "# Example usage (set a real image path):\n",
        "result = infer_image(model, \"data/processed/sample.png\", config, device)\n",
        "print(result)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
